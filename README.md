# MSCOCO CLIP Distillation Project

In this project, we study the distillation of the CLIP (Contrastive Language-Image Pretraining) model. The teacher model is a pretrained ResNet-50, while the student model is an untrained ResNet-34, initialized with random weights and lacking the learned feature representations required for effective image recognition tasks. We propose a novel loss function for CLIP model distillation that incorporates synergetic and redundant information, inspired by Partial Information Decomposition (PID). This loss function enhances the student modelâ€™s ability to capture both unique and shared information across modalities, improving performance on image-to-text (I2T) and text-to-image (T2I) retrieval tasks. We compare the performance of our student model under four different loss configurations: a simple baseline model using mean square error, a strong baseline model utilizing Contrastive Loss, an Extension 1 model that integrates synergetic information from images and text, and an Extension 2 model that further incorporates redundant information. Experimental results demonstrate that the Extension 2 model achieves the best performance, highlighting the efficacy of our proposed approach.
