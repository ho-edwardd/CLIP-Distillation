{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdPysIbDhFsa",
        "outputId": "50aa74bc-74cb-4c0f-e3a3-77db4e44ac85"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet ftfy regex tqdm\n",
        "!pip install --quiet git+https://github.com/openai/CLIP.git\n",
        "!pip install --quiet pycocotools\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download and Prepare the MS COCO Dataset"
      ],
      "metadata": {
        "id": "OQ5eAnW_lmiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from zipfile import ZipFile\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define paths\n",
        "data_dir = '/content/coco2014'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# URLs for datasets and annotations\n",
        "datasets = {\n",
        "    \"train2014\": \"http://images.cocodataset.org/zips/train2014.zip\",\n",
        "    \"val2014\": \"http://images.cocodataset.org/zips/val2014.zip\",\n",
        "    \"annotations_trainval2014\": \"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\",\n",
        "}\n",
        "\n",
        "# Download helper function with progress bar\n",
        "def download_file(url, dest_path):\n",
        "    response = requests.get(url, stream=True)\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "    with open(dest_path, 'wb') as f, tqdm(\n",
        "        desc=f\"Downloading {os.path.basename(dest_path)}\",\n",
        "        total=total_size,\n",
        "        unit='B',\n",
        "        unit_scale=True,\n",
        "        unit_divisor=1024\n",
        "    ) as bar:\n",
        "        for data in response.iter_content(chunk_size=1024):\n",
        "            f.write(data)\n",
        "            bar.update(len(data))\n",
        "\n",
        "# Download and extract datasets\n",
        "for name, url in datasets.items():\n",
        "    zip_path = os.path.join(data_dir, f\"{name}.zip\")\n",
        "    print(f\"Processing {name}...\")\n",
        "\n",
        "    # Download the dataset\n",
        "    download_file(url, zip_path)\n",
        "\n",
        "    # Unzip the dataset\n",
        "    with ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(data_dir)\n",
        "\n",
        "    # Remove the zip file to save space\n",
        "    os.remove(zip_path)\n",
        "    print(f\"{name} downloaded and extracted.\")\n",
        "\n",
        "print(\"All datasets and annotations successfully downloaded and extracted!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XMDkrWBhLGg",
        "outputId": "e2df9e01-9339-41e4-841e-5c0a2e114ca9"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing train2014...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading train2014.zip: 100%|██████████| 12.6G/12.6G [14:25<00:00, 15.6MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train2014 downloaded and extracted.\n",
            "Processing val2014...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading val2014.zip: 100%|██████████| 6.19G/6.19G [07:49<00:00, 14.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val2014 downloaded and extracted.\n",
            "Processing annotations_trainval2014...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading annotations_trainval2014.zip: 100%|██████████| 241M/241M [00:19<00:00, 12.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "annotations_trainval2014 downloaded and extracted.\n",
            "All datasets and annotations successfully downloaded and extracted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Teacher Model: CLIP RN50 Model"
      ],
      "metadata": {
        "id": "FMucybFulqLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import clip\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "# Load the CLIP model\n",
        "model, preprocess = clip.load(\"RN50\", device)\n",
        "model.eval()\n",
        "\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)\n"
      ],
      "metadata": {
        "id": "NtcJ2B3fhLfo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "00f9c523-c968-4c10-c013-0e69151b6947"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 244M/244M [00:08<00:00, 31.5MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 102,007,137\n",
            "Input resolution: 224\n",
            "Context length: 77\n",
            "Vocab size: 49408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Student Model (ResNet-34)"
      ],
      "metadata": {
        "id": "GifHBhlWlr29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Student Image Encoder (ResNet-34)\n",
        "class StudentImageEncoder(nn.Module):\n",
        "    def __init__(self, output_dim):\n",
        "        super(StudentImageEncoder, self).__init__()\n",
        "        self.encoder = models.resnet34(pretrained=False)\n",
        "        self.encoder.fc = nn.Linear(self.encoder.fc.in_features, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = x / x.norm(dim=-1, keepdim=True)  # Normalize\n",
        "        return x\n",
        "\n",
        "\n",
        "class StudentTextEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, context_length, output_dim):\n",
        "        super(StudentTextEncoder, self).__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, output_dim)\n",
        "        self.positional_embedding = nn.Parameter(torch.zeros(context_length, output_dim))\n",
        "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=output_dim, nhead=8)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "        self.ln_final = nn.LayerNorm(output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len)\n",
        "        x = self.token_embedding(x) + self.positional_embedding  # (batch_size, seq_len, output_dim)\n",
        "        x = x.permute(1, 0, 2)  # (seq_len, batch_size, output_dim)\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)  # (batch_size, seq_len, output_dim)\n",
        "        x = self.ln_final(x)\n",
        "        x = x.mean(dim=1)  # Mean pooling over the sequence length\n",
        "        x = x / x.norm(dim=-1, keepdim=True)  # Normalize to unit length\n",
        "        return x  # (batch_size, output_dim)\n",
        "\n"
      ],
      "metadata": {
        "id": "Bu5lXd8ehL2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the MSCOCO Data Loaders"
      ],
      "metadata": {
        "id": "h_cfc6FcltX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Preprocessing transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((input_resolution, input_resolution)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
        "                         std=(0.26862954, 0.26130258, 0.27577711))\n",
        "])\n",
        "\n",
        "# Custom dataset to select one caption per image\n",
        "class CocoDataset(Dataset):\n",
        "    def __init__(self, root, annFile, transform=None):\n",
        "        self.dataset = datasets.CocoCaptions(root=root, annFile=annFile, transform=transform)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, captions = self.dataset[idx]\n",
        "        # Select the first caption\n",
        "        caption = captions[0]\n",
        "        # Tokenize the caption\n",
        "        text = clip.tokenize(caption, context_length=context_length)[0]\n",
        "        return image, text\n",
        "\n",
        "# Paths to images and annotations\n",
        "train_img_dir = os.path.join(data_dir, 'train2014')\n",
        "train_ann_file = os.path.join(data_dir, 'annotations', 'captions_train2014.json')\n",
        "\n",
        "# Create the training dataset and dataloader\n",
        "train_dataset = CocoDataset(root=train_img_dir, annFile=train_ann_file, transform=transform)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,  # batch size\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "dGosFCtHhMMi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5f1fd6fe-2539-4ead-993b-17800505aba4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.66s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Loss Function Using MSE"
      ],
      "metadata": {
        "id": "KijNXkynlvLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "def loss_with_l2(\n",
        "    student_image_features,\n",
        "    student_text_features,\n",
        "    teacher_image_features,\n",
        "    teacher_text_features\n",
        "):\n",
        "\n",
        "\n",
        "    # Normalize features\n",
        "    student_image_features = student_image_features / student_image_features.norm(dim=-1, keepdim=True)\n",
        "    student_text_features = student_text_features / student_text_features.norm(dim=-1, keepdim=True)\n",
        "    teacher_image_features = teacher_image_features / teacher_image_features.norm(dim=-1, keepdim=True)\n",
        "    teacher_text_features = teacher_text_features / teacher_text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "\n",
        "    # L2 distance loss\n",
        "    l2_img = F.mse_loss(student_image_features, teacher_image_features)\n",
        "    l2_txt = F.mse_loss(student_text_features, teacher_text_features)\n",
        "    l2_loss = (l2_img + l2_txt) / 2\n",
        "\n",
        "\n",
        "    total_loss = l2_loss\n",
        "\n",
        "    return total_loss\n"
      ],
      "metadata": {
        "id": "adWN2SVlXyrb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up the Training Loop"
      ],
      "metadata": {
        "id": "BijGiufjlw8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate student models\n",
        "student_image_encoder = StudentImageEncoder(output_dim=1024).to(device)\n",
        "student_text_encoder = StudentTextEncoder(vocab_size, context_length, output_dim=1024).to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(student_image_encoder.parameters()) + list(student_text_encoder.parameters()),\n",
        "    lr=1e-4\n",
        ")\n"
      ],
      "metadata": {
        "id": "UyA-cl0LhM6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "eda56544-7de3-428e-fe75-c492acdb5cc6"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Student Model"
      ],
      "metadata": {
        "id": "0isNcb4ClyWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "num_epochs = 5  # the number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    student_image_encoder.train()\n",
        "    student_text_encoder.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, texts) in enumerate(train_dataloader):\n",
        "        images = images.to(device)\n",
        "        texts = texts.to(device)\n",
        "\n",
        "        # Teacher outputs\n",
        "        with torch.no_grad():\n",
        "            teacher_image_features = model.encode_image(images)\n",
        "            teacher_text_features = model.encode_text(texts)\n",
        "\n",
        "        # Student outputs\n",
        "        student_image_features = student_image_encoder(images).to(teacher_image_features.dtype)\n",
        "        student_text_features = student_text_encoder(texts).to(teacher_text_features.dtype)\n",
        "\n",
        "        # Compute Loss with detailed returns\n",
        "        loss = loss_with_l2(\n",
        "            student_image_features,\n",
        "            student_text_features,\n",
        "            teacher_image_features,\n",
        "            teacher_text_features\n",
        "        )\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(\n",
        "                f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_dataloader)}], \"\n",
        "                f\"Loss: {loss.item():.10f}\"\n",
        "            )\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dEFrE9NeCcJ2",
        "outputId": "aeca1a62-f466-429d-a23b-86f0f1c6597c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [0/1294], Loss: 0.0006260872\n",
            "Epoch [1/5], Step [100/1294], Loss: 0.0006008148\n",
            "Epoch [1/5], Step [200/1294], Loss: 0.0005578995\n",
            "Epoch [1/5], Step [300/1294], Loss: 0.0004940033\n",
            "Epoch [1/5], Step [400/1294], Loss: 0.0004930496\n",
            "Epoch [1/5], Step [500/1294], Loss: 0.0004553795\n",
            "Epoch [1/5], Step [600/1294], Loss: 0.0004348755\n",
            "Epoch [1/5], Step [700/1294], Loss: 0.0004620552\n",
            "Epoch [1/5], Step [800/1294], Loss: 0.0004358292\n",
            "Epoch [1/5], Step [900/1294], Loss: 0.0004110336\n",
            "Epoch [1/5], Step [1000/1294], Loss: 0.0004281998\n",
            "Epoch [1/5], Step [1100/1294], Loss: 0.0004146099\n",
            "Epoch [1/5], Step [1200/1294], Loss: 0.0004065037\n",
            "Epoch [1/5], Average Loss: 0.0005\n",
            "Epoch [2/5], Step [0/1294], Loss: 0.0004043579\n",
            "Epoch [2/5], Step [100/1294], Loss: 0.0004079342\n",
            "Epoch [2/5], Step [200/1294], Loss: 0.0004165173\n",
            "Epoch [2/5], Step [300/1294], Loss: 0.0003681183\n",
            "Epoch [2/5], Step [400/1294], Loss: 0.0003802776\n",
            "Epoch [2/5], Step [500/1294], Loss: 0.0003755093\n",
            "Epoch [2/5], Step [600/1294], Loss: 0.0003716946\n",
            "Epoch [2/5], Step [700/1294], Loss: 0.0003724098\n",
            "Epoch [2/5], Step [800/1294], Loss: 0.0003557205\n",
            "Epoch [2/5], Step [900/1294], Loss: 0.0003929138\n",
            "Epoch [2/5], Step [1000/1294], Loss: 0.0003826618\n",
            "Epoch [2/5], Step [1100/1294], Loss: 0.0003294945\n",
            "Epoch [2/5], Step [1200/1294], Loss: 0.0003557205\n",
            "Epoch [2/5], Average Loss: 0.0004\n",
            "Epoch [3/5], Step [0/1294], Loss: 0.0003495216\n",
            "Epoch [3/5], Step [100/1294], Loss: 0.0003530979\n",
            "Epoch [3/5], Step [200/1294], Loss: 0.0003664494\n",
            "Epoch [3/5], Step [300/1294], Loss: 0.0003437996\n",
            "Epoch [3/5], Step [400/1294], Loss: 0.0003533363\n",
            "Epoch [3/5], Step [500/1294], Loss: 0.0003275871\n",
            "Epoch [3/5], Step [600/1294], Loss: 0.0003471375\n",
            "Epoch [3/5], Step [700/1294], Loss: 0.0003430843\n",
            "Epoch [3/5], Step [800/1294], Loss: 0.0003116131\n",
            "Epoch [3/5], Step [900/1294], Loss: 0.0003407001\n",
            "Epoch [3/5], Step [1000/1294], Loss: 0.0003035069\n",
            "Epoch [3/5], Step [1100/1294], Loss: 0.0003390312\n",
            "Epoch [3/5], Step [1200/1294], Loss: 0.0003473759\n",
            "Epoch [3/5], Average Loss: 0.0003\n",
            "Epoch [4/5], Step [0/1294], Loss: 0.0003085136\n",
            "Epoch [4/5], Step [100/1294], Loss: 0.0003483295\n",
            "Epoch [4/5], Step [200/1294], Loss: 0.0003488064\n",
            "Epoch [4/5], Step [300/1294], Loss: 0.0003302097\n",
            "Epoch [4/5], Step [400/1294], Loss: 0.0003094673\n",
            "Epoch [4/5], Step [500/1294], Loss: 0.0003159046\n",
            "Epoch [4/5], Step [600/1294], Loss: 0.0003094673\n",
            "Epoch [4/5], Step [700/1294], Loss: 0.0003051758\n",
            "Epoch [4/5], Step [800/1294], Loss: 0.0003280640\n",
            "Epoch [4/5], Step [900/1294], Loss: 0.0003082752\n",
            "Epoch [4/5], Step [1000/1294], Loss: 0.0003068447\n",
            "Epoch [4/5], Step [1100/1294], Loss: 0.0003168583\n",
            "Epoch [4/5], Step [1200/1294], Loss: 0.0002944469\n",
            "Epoch [4/5], Average Loss: 0.0003\n",
            "Epoch [5/5], Step [0/1294], Loss: 0.0003054142\n",
            "Epoch [5/5], Step [100/1294], Loss: 0.0003004074\n",
            "Epoch [5/5], Step [200/1294], Loss: 0.0003080368\n",
            "Epoch [5/5], Step [300/1294], Loss: 0.0002958775\n",
            "Epoch [5/5], Step [400/1294], Loss: 0.0003037453\n",
            "Epoch [5/5], Step [500/1294], Loss: 0.0003056526\n",
            "Epoch [5/5], Step [600/1294], Loss: 0.0002975464\n",
            "Epoch [5/5], Step [700/1294], Loss: 0.0003044605\n",
            "Epoch [5/5], Step [800/1294], Loss: 0.0002965927\n",
            "Epoch [5/5], Step [900/1294], Loss: 0.0002918243\n",
            "Epoch [5/5], Step [1000/1294], Loss: 0.0002937317\n",
            "Epoch [5/5], Step [1100/1294], Loss: 0.0002689362\n",
            "Epoch [5/5], Step [1200/1294], Loss: 0.0002894402\n",
            "Epoch [5/5], Average Loss: 0.0003\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the Trained Student Model"
      ],
      "metadata": {
        "id": "Q_tN8A1Cyzzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset\n",
        "import clip\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "input_resolution = 224\n",
        "context_length = 77\n",
        "\n",
        "# Evaluation transforms (same as training)\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((input_resolution, input_resolution)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
        "                         std=(0.26862954, 0.26130258, 0.27577711))\n",
        "])\n",
        "\n",
        "class CocoEvalDataset(Dataset):\n",
        "    def __init__(self, root, annFile, transform=None):\n",
        "        self.dataset = datasets.CocoCaptions(root=root, annFile=annFile, transform=transform)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, captions = self.dataset[idx]\n",
        "        # Return the full list of captions for each image\n",
        "        return image, captions\n",
        "\n",
        "def coco_collate_fn(batch):\n",
        "    # batch is a list of (image, captions_list) tuples\n",
        "    images = []\n",
        "    captions = []\n",
        "    for img, caps in batch:\n",
        "        images.append(img)      # img is a Tensor\n",
        "        captions.append(caps)   # caps is a list of strings\n",
        "    images = torch.stack(images, dim=0)  # stack all images into a single tensor\n",
        "    return images, captions\n",
        "\n",
        "\n",
        "# Paths for validation\n",
        "val_img_dir = os.path.join(data_dir, 'val2014')\n",
        "val_ann_file = os.path.join(data_dir, 'annotations', 'captions_val2014.json')\n",
        "\n",
        "\n",
        "\n",
        "val_dataset = CocoEvalDataset(root=val_img_dir, annFile=val_ann_file, transform=eval_transform)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2, collate_fn=coco_collate_fn)\n",
        "\n",
        "\n",
        "student_image_encoder.eval()\n",
        "student_text_encoder.eval()\n",
        "\n",
        "all_image_features = []\n",
        "all_text_features = []\n",
        "image_to_text_indices = []  # For each image, store which text indices correspond to its captions\n",
        "all_captions_flat = []  # We'll store all captions globally\n",
        "\n",
        "with torch.no_grad():\n",
        "    image_count = 0\n",
        "    text_count = 0\n",
        "    for images, batch_captions in val_dataloader:\n",
        "        # images: (B, C, H, W)\n",
        "        # batch_captions: list of length B, each item is a list of captions for that image\n",
        "\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Encode images\n",
        "        image_feats = student_image_encoder(images)\n",
        "        image_feats = image_feats / image_feats.norm(dim=-1, keepdim=True)\n",
        "        all_image_features.append(image_feats.cpu())\n",
        "\n",
        "        # Flatten captions for this batch\n",
        "        flat_captions = []\n",
        "        image_to_text_map_for_batch = []\n",
        "        for caps in batch_captions:\n",
        "            start_idx = len(flat_captions)\n",
        "            flat_captions.extend(caps)  # add all captions from this image\n",
        "            end_idx = len(flat_captions)\n",
        "            # This image's captions correspond to indices [start_idx+text_count, end_idx+text_count)\n",
        "            image_to_text_map_for_batch.append((start_idx + text_count, end_idx + text_count))\n",
        "\n",
        "        # Tokenize all captions in the batch at once\n",
        "        texts = clip.tokenize(flat_captions, context_length=context_length).to(device)\n",
        "        text_feats = student_text_encoder(texts)\n",
        "        text_feats = text_feats / text_feats.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Store text features globally\n",
        "        all_text_features.append(text_feats.cpu())\n",
        "        all_captions_flat.extend(flat_captions)\n",
        "\n",
        "        # Update the global mapping\n",
        "        for (start_idx, end_idx) in image_to_text_map_for_batch:\n",
        "            image_to_text_indices.append(list(range(start_idx, end_idx)))\n",
        "\n",
        "        image_count += images.size(0)\n",
        "        text_count += len(flat_captions)\n",
        "\n",
        "all_image_features = torch.cat(all_image_features, dim=0)  # (N_images, embed_dim)\n",
        "all_text_features = torch.cat(all_text_features, dim=0)    # (N_captions_total, embed_dim)\n",
        "\n",
        "# Compute similarity matrix: shape (N_images, N_captions_total)\n",
        "sim_matrix = all_image_features @ all_text_features.t()\n",
        "\n",
        "def compute_recall_with_multiple_captions(sim_matrix, image_to_text_indices, k=1):\n",
        "    n = sim_matrix.size(0)\n",
        "    successes = 0\n",
        "    for i in range(n):\n",
        "        scores = sim_matrix[i]\n",
        "        sorted_indices = torch.argsort(scores, descending=True)\n",
        "\n",
        "        correct_indices = set(image_to_text_indices[i])\n",
        "        ranks_of_correct = []\n",
        "        for cidx in correct_indices:\n",
        "            pos = (sorted_indices == cidx).nonzero(as_tuple=True)\n",
        "            if len(pos) > 0:\n",
        "                ranks_of_correct.append(pos[0].item())\n",
        "\n",
        "        if len(ranks_of_correct) > 0:\n",
        "            min_rank = min(ranks_of_correct)\n",
        "            if min_rank < k:\n",
        "                successes += 1\n",
        "    recall = successes / n\n",
        "    return recall\n",
        "\n",
        "# Image-to-Text Retrieval\n",
        "r1 = compute_recall_with_multiple_captions(sim_matrix, image_to_text_indices, k=1)\n",
        "r5 = compute_recall_with_multiple_captions(sim_matrix, image_to_text_indices, k=5)\n",
        "r10 = compute_recall_with_multiple_captions(sim_matrix, image_to_text_indices, k=10)\n",
        "\n",
        "print(\"Image-to-Text Retrieval:\")\n",
        "print(f\"Recall@1: {r1*100:.2f}%\")\n",
        "print(f\"Recall@5: {r5*100:.2f}%\")\n",
        "print(f\"Recall@10: {r10*100:.2f}%\")\n",
        "\n",
        "# Text-to-Image Retrieval\n",
        "# Create reverse mapping from text index to image index\n",
        "text_to_image = [None]*all_text_features.size(0)\n",
        "for i, tinds in enumerate(image_to_text_indices):\n",
        "    for t in tinds:\n",
        "        text_to_image[t] = i\n",
        "\n",
        "sim_matrix_t2i = sim_matrix.t()  # (N_captions_total, N_images)\n",
        "\n",
        "def compute_recall_text_to_image(sim_matrix_t2i, text_to_image, k=1):\n",
        "    m = sim_matrix_t2i.size(0)\n",
        "    successes = 0\n",
        "    for j in range(m):\n",
        "        scores = sim_matrix_t2i[j]\n",
        "        sorted_indices = torch.argsort(scores, descending=True)\n",
        "        correct_image = text_to_image[j]\n",
        "        rank = (sorted_indices == correct_image).nonzero(as_tuple=True)[0].item()\n",
        "        if rank < k:\n",
        "            successes += 1\n",
        "    recall = successes / m\n",
        "    return recall\n",
        "\n",
        "r1_t2i = compute_recall_text_to_image(sim_matrix_t2i, text_to_image, k=1)\n",
        "r5_t2i = compute_recall_text_to_image(sim_matrix_t2i, text_to_image, k=5)\n",
        "r10_t2i = compute_recall_text_to_image(sim_matrix_t2i, text_to_image, k=10)\n",
        "\n",
        "print(\"Text-to-Image Retrieval:\")\n",
        "print(f\"Recall@1: {r1_t2i*100:.2f}%\")\n",
        "print(f\"Recall@5: {r5_t2i*100:.2f}%\")\n",
        "print(f\"Recall@10: {r10_t2i*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "FUNccvGF2YsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "94259f85-e35e-43fa-f03c-c3015fc00447"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.51s)\n",
            "creating index...\n",
            "index created!\n",
            "Image-to-Text Retrieval:\n",
            "Recall@1: 0.22%\n",
            "Recall@5: 0.97%\n",
            "Recall@10: 1.83%\n",
            "Text-to-Image Retrieval:\n",
            "Recall@1: 0.33%\n",
            "Recall@5: 1.39%\n",
            "Recall@10: 2.50%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d42i-P_Yrq-Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}