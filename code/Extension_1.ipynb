{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdPysIbDhFsa",
        "outputId": "f0ecfdcb-4a8a-4b73-9a34-5d48b668eef6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet ftfy regex tqdm\n",
        "!pip install --quiet git+https://github.com/openai/CLIP.git\n",
        "!pip install --quiet pycocotools\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download and Prepare the MS COCO Dataset"
      ],
      "metadata": {
        "id": "OQ5eAnW_lmiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from zipfile import ZipFile\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define paths\n",
        "data_dir = '/content/coco2014'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# URLs for datasets and annotations\n",
        "datasets = {\n",
        "    \"train2014\": \"http://images.cocodataset.org/zips/train2014.zip\",\n",
        "    \"val2014\": \"http://images.cocodataset.org/zips/val2014.zip\",\n",
        "    \"annotations_trainval2014\": \"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\",\n",
        "}\n",
        "\n",
        "# Download helper function with progress bar\n",
        "def download_file(url, dest_path):\n",
        "    response = requests.get(url, stream=True)\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "    with open(dest_path, 'wb') as f, tqdm(\n",
        "        desc=f\"Downloading {os.path.basename(dest_path)}\",\n",
        "        total=total_size,\n",
        "        unit='B',\n",
        "        unit_scale=True,\n",
        "        unit_divisor=1024\n",
        "    ) as bar:\n",
        "        for data in response.iter_content(chunk_size=1024):\n",
        "            f.write(data)\n",
        "            bar.update(len(data))\n",
        "\n",
        "# Download and extract datasets\n",
        "for name, url in datasets.items():\n",
        "    zip_path = os.path.join(data_dir, f\"{name}.zip\")\n",
        "    print(f\"Processing {name}...\")\n",
        "\n",
        "    # Download the dataset\n",
        "    download_file(url, zip_path)\n",
        "\n",
        "    # Unzip the dataset\n",
        "    with ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(data_dir)\n",
        "\n",
        "    # Remove the zip file to save space\n",
        "    os.remove(zip_path)\n",
        "    print(f\"{name} downloaded and extracted.\")\n",
        "\n",
        "print(\"All datasets and annotations successfully downloaded and extracted!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XMDkrWBhLGg",
        "outputId": "07e2e9b4-4307-44a4-a4ac-5fd5bfd3336c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing train2014...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading train2014.zip: 100%|██████████| 12.6G/12.6G [03:32<00:00, 63.5MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train2014 downloaded and extracted.\n",
            "Processing val2014...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading val2014.zip: 100%|██████████| 6.19G/6.19G [01:38<00:00, 67.2MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val2014 downloaded and extracted.\n",
            "Processing annotations_trainval2014...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading annotations_trainval2014.zip: 100%|██████████| 241M/241M [00:03<00:00, 73.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "annotations_trainval2014 downloaded and extracted.\n",
            "All datasets and annotations successfully downloaded and extracted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Teacher Model: CLIP RN50 Model"
      ],
      "metadata": {
        "id": "FMucybFulqLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import clip\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "# Load the CLIP model\n",
        "model, preprocess = clip.load(\"RN50\", device)\n",
        "model.eval()\n",
        "\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)\n"
      ],
      "metadata": {
        "id": "NtcJ2B3fhLfo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "137e6a77-4047-440a-8bcf-38fad6853407"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|███████████████████████████████████████| 244M/244M [00:05<00:00, 46.2MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 102,007,137\n",
            "Input resolution: 224\n",
            "Context length: 77\n",
            "Vocab size: 49408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Student Model (ResNet-34)"
      ],
      "metadata": {
        "id": "GifHBhlWlr29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Student Image Encoder (ResNet-34)\n",
        "class StudentImageEncoder(nn.Module):\n",
        "    def __init__(self, output_dim):\n",
        "        super(StudentImageEncoder, self).__init__()\n",
        "        self.encoder = models.resnet34(pretrained=False)\n",
        "        self.encoder.fc = nn.Linear(self.encoder.fc.in_features, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = x / x.norm(dim=-1, keepdim=True)  # Normalize\n",
        "        return x\n",
        "\n",
        "\n",
        "class StudentTextEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, context_length, output_dim):\n",
        "        super(StudentTextEncoder, self).__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, output_dim)\n",
        "        self.positional_embedding = nn.Parameter(torch.zeros(context_length, output_dim))\n",
        "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=output_dim, nhead=8)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "        self.ln_final = nn.LayerNorm(output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len)\n",
        "        x = self.token_embedding(x) + self.positional_embedding  # (batch_size, seq_len, output_dim)\n",
        "        x = x.permute(1, 0, 2)  # (seq_len, batch_size, output_dim)\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)  # (batch_size, seq_len, output_dim)\n",
        "        x = self.ln_final(x)\n",
        "        x = x.mean(dim=1)  # Mean pooling over the sequence length\n",
        "        x = x / x.norm(dim=-1, keepdim=True)  # Normalize to unit length\n",
        "        return x  # (batch_size, output_dim)\n",
        "\n"
      ],
      "metadata": {
        "id": "Bu5lXd8ehL2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the MSCOCO Data Loaders"
      ],
      "metadata": {
        "id": "h_cfc6FcltX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Preprocessing transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((input_resolution, input_resolution)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
        "                         std=(0.26862954, 0.26130258, 0.27577711))\n",
        "])\n",
        "\n",
        "# Custom dataset to select one caption per image\n",
        "class CocoDataset(Dataset):\n",
        "    def __init__(self, root, annFile, transform=None):\n",
        "        self.dataset = datasets.CocoCaptions(root=root, annFile=annFile, transform=transform)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, captions = self.dataset[idx]\n",
        "        # Select the first caption\n",
        "        caption = captions[0]\n",
        "        # Tokenize the caption\n",
        "        text = clip.tokenize(caption, context_length=context_length)[0]\n",
        "        return image, text\n",
        "\n",
        "# Paths to images and annotations\n",
        "train_img_dir = os.path.join(data_dir, 'train2014')\n",
        "train_ann_file = os.path.join(data_dir, 'annotations', 'captions_train2014.json')\n",
        "\n",
        "# Create the training dataset and dataloader\n",
        "train_dataset = CocoDataset(root=train_img_dir, annFile=train_ann_file, transform=transform)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,  # batch size\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n"
      ],
      "metadata": {
        "id": "dGosFCtHhMMi",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ff0c2947-5144-4817-bbef-c0fa840498f5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.61s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Contrastive Loss Function Using Logit + KL + MSE"
      ],
      "metadata": {
        "id": "KijNXkynlvLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "def contrastive_loss_with_kl_l2(\n",
        "    student_image_features,\n",
        "    student_text_features,\n",
        "    teacher_image_features,\n",
        "    teacher_text_features,\n",
        "    temperature=0.07,\n",
        "    alpha=0.5,  # weight for KL term\n",
        "    beta=0.5    # weight for L2 term\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute a combined loss:\n",
        "    - Contrastive loss for image-text alignment,\n",
        "    - KL-divergence to match the student distributions with the teacher distributions,\n",
        "    - L2 distance (feature-level distillation) between student and teacher embeddings.\n",
        "\n",
        "    Args:\n",
        "        student_image_features: Tensor of shape (batch_size, embed_dim) for student images.\n",
        "        student_text_features: Tensor of shape (batch_size, embed_dim) for student text.\n",
        "        teacher_image_features: Tensor of shape (batch_size, embed_dim) for teacher images.\n",
        "        teacher_text_features: Tensor of shape (batch_size, embed_dim) for teacher text.\n",
        "        temperature: Temperature for scaling logits (default=0.07).\n",
        "        alpha: Weight for the KL-divergence loss.\n",
        "        beta: Weight for the L2 distance loss.\n",
        "\n",
        "    Returns:\n",
        "        total_loss: Combined loss value.\n",
        "    \"\"\"\n",
        "\n",
        "    # Normalize features\n",
        "    student_image_features = student_image_features / student_image_features.norm(dim=-1, keepdim=True)\n",
        "    student_text_features = student_text_features / student_text_features.norm(dim=-1, keepdim=True)\n",
        "    teacher_image_features = teacher_image_features / teacher_image_features.norm(dim=-1, keepdim=True)\n",
        "    teacher_text_features = teacher_text_features / teacher_text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Compute student logits\n",
        "    logits_per_image_student = student_image_features @ student_text_features.t() / temperature\n",
        "    logits_per_text_student = logits_per_image_student.t()\n",
        "\n",
        "    # Compute teacher logits (no gradients)\n",
        "    with torch.no_grad():\n",
        "        logits_per_image_teacher = teacher_image_features @ teacher_text_features.t() / temperature\n",
        "        logits_per_text_teacher = logits_per_image_teacher.t()\n",
        "\n",
        "    # Contrastive loss (as before)\n",
        "    batch_size = student_image_features.size(0)\n",
        "    labels = torch.arange(batch_size, device=student_image_features.device)\n",
        "    loss_image = F.cross_entropy(logits_per_image_student, labels)\n",
        "    loss_text = F.cross_entropy(logits_per_text_student, labels)\n",
        "    contrastive_loss = (loss_image + loss_text) / 2\n",
        "\n",
        "    # KL-divergence loss\n",
        "    # Use softmax for teacher and log_softmax for student\n",
        "    student_img_log_probs = F.log_softmax(logits_per_image_student, dim=-1)\n",
        "    teacher_img_probs = F.softmax(logits_per_image_teacher, dim=-1)\n",
        "    kl_img = F.kl_div(student_img_log_probs, teacher_img_probs, reduction='batchmean')\n",
        "\n",
        "    student_txt_log_probs = F.log_softmax(logits_per_text_student, dim=-1)\n",
        "    teacher_txt_probs = F.softmax(logits_per_text_teacher, dim=-1)\n",
        "    kl_txt = F.kl_div(student_txt_log_probs, teacher_txt_probs, reduction='batchmean')\n",
        "\n",
        "    kl_loss = (kl_img + kl_txt) / 2\n",
        "\n",
        "    # L2 distance loss (feature-level distillation)\n",
        "    # We can use MSE between corresponding embeddings\n",
        "    l2_img = F.mse_loss(student_image_features, teacher_image_features)\n",
        "    l2_txt = F.mse_loss(student_text_features, teacher_text_features)\n",
        "    l2_loss = (l2_img + l2_txt) / 2\n",
        "\n",
        "    # Combine all losses\n",
        "    total_loss = contrastive_loss + alpha * kl_loss + beta * l2_loss\n",
        "\n",
        "    return total_loss\n"
      ],
      "metadata": {
        "id": "9L52QSPmA0aF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torch.nn as nn\n",
        "\n",
        "def compute_redundancy(student_image_features, student_text_features,\n",
        "                       teacher_image_features, teacher_text_features):\n",
        "    # Normalize\n",
        "    s_i = F.normalize(student_image_features, dim=-1)\n",
        "    s_t = F.normalize(student_text_features, dim=-1)\n",
        "    t_i = F.normalize(teacher_image_features, dim=-1)\n",
        "    t_t = F.normalize(teacher_text_features, dim=-1)\n",
        "\n",
        "    # Approximate redundancy as the average similarity of corresponding modalities\n",
        "    img_similarity = (s_i * t_i).sum(dim=-1).mean()  # dot product similarity\n",
        "    txt_similarity = (s_t * t_t).sum(dim=-1).mean()\n",
        "    redundancy = (img_similarity + txt_similarity) / 2.0\n",
        "    return redundancy\n",
        "\n",
        "def compute_synergy(student_image_features, student_text_features,\n",
        "                    teacher_image_features, teacher_text_features):\n",
        "    # Normalize\n",
        "    s_i = F.normalize(student_image_features, dim=-1)\n",
        "    s_t = F.normalize(student_text_features, dim=-1)\n",
        "    t_i = F.normalize(teacher_image_features, dim=-1)\n",
        "    t_t = F.normalize(teacher_text_features, dim=-1)\n",
        "\n",
        "    # Joint embeddings (concatenate image and text)\n",
        "    s_joint = torch.cat([s_i, s_t], dim=-1)\n",
        "    t_joint = torch.cat([t_i, t_t], dim=-1)\n",
        "\n",
        "    def avg_cosine(u, v):\n",
        "        return (u * v).sum(dim=-1).mean()\n",
        "\n",
        "    joint_similarity = avg_cosine(s_joint, t_joint)\n",
        "    img_similarity = avg_cosine(s_i, t_i)\n",
        "    txt_similarity = avg_cosine(s_t, t_t)\n",
        "\n",
        "    # Synergy heuristic: joint similarity minus the average of separate similarities\n",
        "    synergy = joint_similarity - (img_similarity + txt_similarity) / 2.0\n",
        "    return synergy\n",
        "\n",
        "def contrastive_loss_with_kl_l2_pid(\n",
        "    student_image_features,\n",
        "    student_text_features,\n",
        "    teacher_image_features,\n",
        "    teacher_text_features,\n",
        "    temperature=0.07,\n",
        "    alpha=1.0,  # weight for KL term\n",
        "    beta=0.3,   # weight for L2 term\n",
        "    gamma=0.7,  # weight for synergy rewards\n",
        "#    delta=0.1   # weight for redundancy reward\n",
        "):\n",
        "    \"\"\"\n",
        "    Compute a combined loss:\n",
        "    - Contrastive loss for image-text alignment,\n",
        "    - KL-divergence to match student distributions with teacher distributions,\n",
        "    - L2 distance (feature-level distillation),\n",
        "    - PID-inspired terms: maximize synergy.\n",
        "\n",
        "    Args:\n",
        "        student_image_features: (batch_size, embed_dim)\n",
        "        student_text_features: (batch_size, embed_dim)\n",
        "        teacher_image_features: (batch_size, embed_dim)\n",
        "        teacher_text_features: (batch_size, embed_dim)\n",
        "        temperature: Temperature for scaling logits (default=0.07).\n",
        "        alpha: Weight for the KL-divergence loss.\n",
        "        beta: Weight for the L2 distance loss.\n",
        "        gamma: Weight for the synergy rewards.\n",
        "        delta: Weight for the redundancy reward.\n",
        "\n",
        "    Returns:\n",
        "        total_loss: Combined loss value.\n",
        "    \"\"\"\n",
        "\n",
        "    # Normalize features\n",
        "    student_image_features = student_image_features / student_image_features.norm(dim=-1, keepdim=True)\n",
        "    student_text_features = student_text_features / student_text_features.norm(dim=-1, keepdim=True)\n",
        "    teacher_image_features = teacher_image_features / teacher_image_features.norm(dim=-1, keepdim=True)\n",
        "    teacher_text_features = teacher_text_features / teacher_text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Compute student logits\n",
        "    logits_per_image_student = student_image_features @ student_text_features.t() / temperature\n",
        "    logits_per_text_student = logits_per_image_student.t()\n",
        "\n",
        "    # Compute teacher logits (no gradients)\n",
        "    with torch.no_grad():\n",
        "        logits_per_image_teacher = teacher_image_features @ teacher_text_features.t() / temperature\n",
        "        logits_per_text_teacher = logits_per_image_teacher.t()\n",
        "\n",
        "    # Contrastive loss\n",
        "    batch_size = student_image_features.size(0)\n",
        "    labels = torch.arange(batch_size, device=student_image_features.device)\n",
        "    loss_image = F.cross_entropy(logits_per_image_student, labels)\n",
        "    loss_text = F.cross_entropy(logits_per_text_student, labels)\n",
        "    contrastive_loss = (loss_image + loss_text) / 2\n",
        "\n",
        "    # KL-divergence loss\n",
        "    student_img_log_probs = F.log_softmax(logits_per_image_student, dim=-1)\n",
        "    teacher_img_probs = F.softmax(logits_per_image_teacher, dim=-1)\n",
        "    kl_img = F.kl_div(student_img_log_probs, teacher_img_probs, reduction='batchmean')\n",
        "\n",
        "    student_txt_log_probs = F.log_softmax(logits_per_text_student, dim=-1)\n",
        "    teacher_txt_probs = F.softmax(logits_per_text_teacher, dim=-1)\n",
        "    kl_txt = F.kl_div(student_txt_log_probs, teacher_txt_probs, reduction='batchmean')\n",
        "\n",
        "    kl_loss = (kl_img + kl_txt) / 2\n",
        "\n",
        "    # L2 distance loss\n",
        "    l2_img = F.mse_loss(student_image_features, teacher_image_features)\n",
        "    l2_txt = F.mse_loss(student_text_features, teacher_text_features)\n",
        "    l2_loss = (l2_img + l2_txt) / 2\n",
        "\n",
        "    # Compute synergy\n",
        "   # redundancy = compute_redundancy(student_image_features, student_text_features,\n",
        "   #                                 teacher_image_features, teacher_text_features)\n",
        "    synergy = compute_synergy(student_image_features, student_text_features,\n",
        "                              teacher_image_features, teacher_text_features)\n",
        "\n",
        "    # Combine all losses\n",
        "    # We add synergy to the loss (want to maximize synergy)\n",
        "#    total_loss = contrastive_loss + alpha * kl_loss + beta * l2_loss - gamma * synergy - delta * redundancy\n",
        "    total_loss = contrastive_loss + alpha * kl_loss + beta * l2_loss - gamma * synergy\n",
        "\n",
        "    return total_loss\n"
      ],
      "metadata": {
        "id": "-GH3X9WbxT0y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up the Training Loop"
      ],
      "metadata": {
        "id": "BijGiufjlw8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate student models\n",
        "student_image_encoder = StudentImageEncoder(output_dim=1024).to(device)\n",
        "student_text_encoder = StudentTextEncoder(vocab_size, context_length, output_dim=1024).to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(student_image_encoder.parameters()) + list(student_text_encoder.parameters()),\n",
        "    lr=1e-4\n",
        ")\n"
      ],
      "metadata": {
        "id": "UyA-cl0LhM6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b587c362-43b5-4614-a45e-39508fa52522"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Student Model"
      ],
      "metadata": {
        "id": "0isNcb4ClyWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "num_epochs = 20  # the number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    student_image_encoder.train()\n",
        "    student_text_encoder.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, texts) in enumerate(train_dataloader):\n",
        "        images = images.to(device)\n",
        "        texts = texts.to(device)\n",
        "\n",
        "        # Teacher outputs\n",
        "        with torch.no_grad():\n",
        "            teacher_image_features = model.encode_image(images)\n",
        "            teacher_text_features = model.encode_text(texts)\n",
        "\n",
        "        # Student outputs\n",
        "        student_image_features = student_image_encoder(images).to(teacher_image_features.dtype)\n",
        "        student_text_features = student_text_encoder(texts).to(teacher_text_features.dtype)\n",
        "\n",
        "        # Compute Contrastive Loss between student features and teacher features\n",
        "        loss = contrastive_loss_with_kl_l2_pid(student_image_features, student_text_features, teacher_image_features, teacher_text_features)\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "#        if batch_idx % 500 == 0:\n",
        "#            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "id": "vD-4WihtsZHe",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07580d36-ab8d-41d9-8ec1-25bada04da04"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/20], Average Loss: 1.1053\n",
            "Epoch [2/20], Average Loss: 1.0505\n",
            "Epoch [3/20], Average Loss: 1.0055\n",
            "Epoch [4/20], Average Loss: 0.9675\n",
            "Epoch [5/20], Average Loss: 0.9335\n",
            "Epoch [6/20], Average Loss: 0.9083\n",
            "Epoch [7/20], Average Loss: 0.8817\n",
            "Epoch [8/20], Average Loss: 0.8642\n",
            "Epoch [9/20], Average Loss: 0.8436\n",
            "Epoch [10/20], Average Loss: 0.8278\n",
            "Epoch [11/20], Average Loss: 0.8117\n",
            "Epoch [12/20], Average Loss: 0.7993\n",
            "Epoch [13/20], Average Loss: 0.7871\n",
            "Epoch [14/20], Average Loss: 0.7739\n",
            "Epoch [15/20], Average Loss: 0.7640\n",
            "Epoch [16/20], Average Loss: 0.7520\n",
            "Epoch [17/20], Average Loss: 0.7434\n",
            "Epoch [18/20], Average Loss: 0.7364\n",
            "Epoch [19/20], Average Loss: 0.7260\n",
            "Epoch [20/20], Average Loss: 0.7207\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the Trained Student Model"
      ],
      "metadata": {
        "id": "Q_tN8A1Cyzzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset\n",
        "import clip\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "input_resolution = 224\n",
        "context_length = 77\n",
        "\n",
        "# Evaluation transforms (same as training)\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((input_resolution, input_resolution)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
        "                         std=(0.26862954, 0.26130258, 0.27577711))\n",
        "])\n",
        "\n",
        "class CocoEvalDataset(Dataset):\n",
        "    def __init__(self, root, annFile, transform=None):\n",
        "        self.dataset = datasets.CocoCaptions(root=root, annFile=annFile, transform=transform)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, captions = self.dataset[idx]\n",
        "        # Return the full list of captions for each image\n",
        "        return image, captions\n",
        "\n",
        "def coco_collate_fn(batch):\n",
        "    # batch is a list of (image, captions_list) tuples\n",
        "    images = []\n",
        "    captions = []\n",
        "    for img, caps in batch:\n",
        "        images.append(img)      # img is a Tensor\n",
        "        captions.append(caps)   # caps is a list of strings\n",
        "    images = torch.stack(images, dim=0)  # stack all images into a single tensor\n",
        "    return images, captions\n",
        "\n",
        "\n",
        "# Paths for validation\n",
        "val_img_dir = os.path.join(data_dir, 'val2014')\n",
        "val_ann_file = os.path.join(data_dir, 'annotations', 'captions_val2014.json')\n",
        "\n",
        "\n",
        "\n",
        "val_dataset = CocoEvalDataset(root=val_img_dir, annFile=val_ann_file, transform=eval_transform)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2, collate_fn=coco_collate_fn)\n",
        "\n",
        "\n",
        "student_image_encoder.eval()\n",
        "student_text_encoder.eval()\n",
        "\n",
        "all_image_features = []\n",
        "all_text_features = []\n",
        "image_to_text_indices = []  # For each image, store which text indices correspond to its captions\n",
        "all_captions_flat = []  # We'll store all captions globally\n",
        "\n",
        "with torch.no_grad():\n",
        "    image_count = 0\n",
        "    text_count = 0\n",
        "    for images, batch_captions in val_dataloader:\n",
        "        # images: (B, C, H, W)\n",
        "        # batch_captions: list of length B, each item is a list of captions for that image\n",
        "\n",
        "        images = images.to(device)\n",
        "\n",
        "        # Encode images\n",
        "        image_feats = student_image_encoder(images)\n",
        "        image_feats = image_feats / image_feats.norm(dim=-1, keepdim=True)\n",
        "        all_image_features.append(image_feats.cpu())\n",
        "\n",
        "        # Flatten captions for this batch\n",
        "        flat_captions = []\n",
        "        image_to_text_map_for_batch = []\n",
        "        for caps in batch_captions:\n",
        "            start_idx = len(flat_captions)\n",
        "            flat_captions.extend(caps)  # add all captions from this image\n",
        "            end_idx = len(flat_captions)\n",
        "            # This image's captions correspond to indices [start_idx+text_count, end_idx+text_count)\n",
        "            image_to_text_map_for_batch.append((start_idx + text_count, end_idx + text_count))\n",
        "\n",
        "        # Tokenize all captions in the batch at once\n",
        "        texts = clip.tokenize(flat_captions, context_length=context_length).to(device)\n",
        "        text_feats = student_text_encoder(texts)\n",
        "        text_feats = text_feats / text_feats.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        # Store text features globally\n",
        "        all_text_features.append(text_feats.cpu())\n",
        "        all_captions_flat.extend(flat_captions)\n",
        "\n",
        "        # Update the global mapping\n",
        "        for (start_idx, end_idx) in image_to_text_map_for_batch:\n",
        "            image_to_text_indices.append(list(range(start_idx, end_idx)))\n",
        "\n",
        "        image_count += images.size(0)\n",
        "        text_count += len(flat_captions)\n",
        "\n",
        "all_image_features = torch.cat(all_image_features, dim=0)  # (N_images, embed_dim)\n",
        "all_text_features = torch.cat(all_text_features, dim=0)    # (N_captions_total, embed_dim)\n",
        "\n",
        "# Compute similarity matrix: shape (N_images, N_captions_total)\n",
        "sim_matrix = all_image_features @ all_text_features.t()\n",
        "\n",
        "def compute_recall_with_multiple_captions(sim_matrix, image_to_text_indices, k=1):\n",
        "    n = sim_matrix.size(0)\n",
        "    successes = 0\n",
        "    for i in range(n):\n",
        "        scores = sim_matrix[i]\n",
        "        sorted_indices = torch.argsort(scores, descending=True)\n",
        "\n",
        "        correct_indices = set(image_to_text_indices[i])\n",
        "        ranks_of_correct = []\n",
        "        for cidx in correct_indices:\n",
        "            pos = (sorted_indices == cidx).nonzero(as_tuple=True)\n",
        "            if len(pos) > 0:\n",
        "                ranks_of_correct.append(pos[0].item())\n",
        "\n",
        "        if len(ranks_of_correct) > 0:\n",
        "            min_rank = min(ranks_of_correct)\n",
        "            if min_rank < k:\n",
        "                successes += 1\n",
        "    recall = successes / n\n",
        "    return recall\n",
        "\n",
        "# Image-to-Text Retrieval\n",
        "r1 = compute_recall_with_multiple_captions(sim_matrix, image_to_text_indices, k=1)\n",
        "r5 = compute_recall_with_multiple_captions(sim_matrix, image_to_text_indices, k=5)\n",
        "r10 = compute_recall_with_multiple_captions(sim_matrix, image_to_text_indices, k=10)\n",
        "\n",
        "print(\"Image-to-Text Retrieval:\")\n",
        "print(f\"Recall@1: {r1*100:.2f}%\")\n",
        "print(f\"Recall@5: {r5*100:.2f}%\")\n",
        "print(f\"Recall@10: {r10*100:.2f}%\")\n",
        "\n",
        "# Text-to-Image Retrieval\n",
        "# Create reverse mapping from text index to image index\n",
        "text_to_image = [None]*all_text_features.size(0)\n",
        "for i, tinds in enumerate(image_to_text_indices):\n",
        "    for t in tinds:\n",
        "        text_to_image[t] = i\n",
        "\n",
        "sim_matrix_t2i = sim_matrix.t()  # (N_captions_total, N_images)\n",
        "\n",
        "def compute_recall_text_to_image(sim_matrix_t2i, text_to_image, k=1):\n",
        "    m = sim_matrix_t2i.size(0)\n",
        "    successes = 0\n",
        "    for j in range(m):\n",
        "        scores = sim_matrix_t2i[j]\n",
        "        sorted_indices = torch.argsort(scores, descending=True)\n",
        "        correct_image = text_to_image[j]\n",
        "        rank = (sorted_indices == correct_image).nonzero(as_tuple=True)[0].item()\n",
        "        if rank < k:\n",
        "            successes += 1\n",
        "    recall = successes / m\n",
        "    return recall\n",
        "\n",
        "r1_t2i = compute_recall_text_to_image(sim_matrix_t2i, text_to_image, k=1)\n",
        "r5_t2i = compute_recall_text_to_image(sim_matrix_t2i, text_to_image, k=5)\n",
        "r10_t2i = compute_recall_text_to_image(sim_matrix_t2i, text_to_image, k=10)\n",
        "\n",
        "print(\"Text-to-Image Retrieval:\")\n",
        "print(f\"Recall@1: {r1_t2i*100:.2f}%\")\n",
        "print(f\"Recall@5: {r5_t2i*100:.2f}%\")\n",
        "print(f\"Recall@10: {r10_t2i*100:.2f}%\")\n"
      ],
      "metadata": {
        "id": "FUNccvGF2YsX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "628c03a2-ada7-454a-f355-a075666ac13f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.40s)\n",
            "creating index...\n",
            "index created!\n",
            "Image-to-Text Retrieval:\n",
            "Recall@1: 1.04%\n",
            "Recall@5: 3.96%\n",
            "Recall@10: 6.66%\n",
            "Text-to-Image Retrieval:\n",
            "Recall@1: 1.00%\n",
            "Recall@5: 3.80%\n",
            "Recall@10: 6.38%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "d42i-P_Yrq-Y"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}