{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WdPysIbDhFsa",
        "outputId": "8aad53de-e126-4e8c-fd68-8ef1023458c0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/44.8 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.8/44.8 kB\u001b[0m \u001b[31m3.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Building wheel for clip (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install --quiet ftfy regex tqdm\n",
        "!pip install --quiet git+https://github.com/openai/CLIP.git\n",
        "!pip install --quiet pycocotools\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Download and Prepare the MS COCO Dataset"
      ],
      "metadata": {
        "id": "OQ5eAnW_lmiK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import requests\n",
        "from zipfile import ZipFile\n",
        "from tqdm import tqdm\n",
        "\n",
        "# Define paths\n",
        "data_dir = '/content/coco2014'\n",
        "os.makedirs(data_dir, exist_ok=True)\n",
        "\n",
        "# URLs for datasets and annotations\n",
        "datasets = {\n",
        "    \"train2014\": \"http://images.cocodataset.org/zips/train2014.zip\",\n",
        "    \"val2014\": \"http://images.cocodataset.org/zips/val2014.zip\",\n",
        "    \"annotations_trainval2014\": \"http://images.cocodataset.org/annotations/annotations_trainval2014.zip\",\n",
        "}\n",
        "\n",
        "# Download helper function with progress bar\n",
        "def download_file(url, dest_path):\n",
        "    response = requests.get(url, stream=True)\n",
        "    total_size = int(response.headers.get('content-length', 0))\n",
        "    with open(dest_path, 'wb') as f, tqdm(\n",
        "        desc=f\"Downloading {os.path.basename(dest_path)}\",\n",
        "        total=total_size,\n",
        "        unit='B',\n",
        "        unit_scale=True,\n",
        "        unit_divisor=1024\n",
        "    ) as bar:\n",
        "        for data in response.iter_content(chunk_size=1024):\n",
        "            f.write(data)\n",
        "            bar.update(len(data))\n",
        "\n",
        "# Download and extract datasets\n",
        "for name, url in datasets.items():\n",
        "    zip_path = os.path.join(data_dir, f\"{name}.zip\")\n",
        "    print(f\"Processing {name}...\")\n",
        "\n",
        "    # Download the dataset\n",
        "    download_file(url, zip_path)\n",
        "\n",
        "    # Unzip the dataset\n",
        "    with ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall(data_dir)\n",
        "\n",
        "    # Remove the zip file to save space\n",
        "    os.remove(zip_path)\n",
        "    print(f\"{name} downloaded and extracted.\")\n",
        "\n",
        "print(\"All datasets and annotations successfully downloaded and extracted!\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9XMDkrWBhLGg",
        "outputId": "3fd99acb-9dc1-40c3-bc2b-06f66d5ea183"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Processing train2014...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading train2014.zip: 100%|██████████| 12.6G/12.6G [15:02<00:00, 15.0MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "train2014 downloaded and extracted.\n",
            "Processing val2014...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading val2014.zip: 100%|██████████| 6.19G/6.19G [07:00<00:00, 15.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "val2014 downloaded and extracted.\n",
            "Processing annotations_trainval2014...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading annotations_trainval2014.zip: 100%|██████████| 241M/241M [00:17<00:00, 14.8MB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "annotations_trainval2014 downloaded and extracted.\n",
            "All datasets and annotations successfully downloaded and extracted!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load the Teacher Model: CLIP RN50 Model"
      ],
      "metadata": {
        "id": "FMucybFulqLQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import clip\n",
        "import numpy as np\n",
        "\n",
        "\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "\n",
        "# Load the CLIP model\n",
        "model, preprocess = clip.load(\"RN50\", device)\n",
        "model.eval()\n",
        "\n",
        "input_resolution = model.visual.input_resolution\n",
        "context_length = model.context_length\n",
        "vocab_size = model.vocab_size\n",
        "\n",
        "print(\"Model parameters:\", f\"{np.sum([int(np.prod(p.shape)) for p in model.parameters()]):,}\")\n",
        "print(\"Input resolution:\", input_resolution)\n",
        "print(\"Context length:\", context_length)\n",
        "print(\"Vocab size:\", vocab_size)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NtcJ2B3fhLfo",
        "outputId": "e67ddbc0-ba8e-4225-fcbd-72f94d10b62c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "100%|████████████████████████████████████████| 244M/244M [00:02<00:00, 125MiB/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model parameters: 102,007,137\n",
            "Input resolution: 224\n",
            "Context length: 77\n",
            "Vocab size: 49408\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Student Model (ResNet-34)"
      ],
      "metadata": {
        "id": "GifHBhlWlr29"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "import torchvision.models as models\n",
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "# Student Image Encoder (ResNet-34)\n",
        "class StudentImageEncoder(nn.Module):\n",
        "    def __init__(self, output_dim):\n",
        "        super(StudentImageEncoder, self).__init__()\n",
        "        self.encoder = models.resnet34(pretrained=False)\n",
        "        self.encoder.fc = nn.Linear(self.encoder.fc.in_features, output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.encoder(x)\n",
        "        x = x / x.norm(dim=-1, keepdim=True)  # Normalize\n",
        "        return x\n",
        "\n",
        "\n",
        "class StudentTextEncoder(nn.Module):\n",
        "    def __init__(self, vocab_size, context_length, output_dim):\n",
        "        super(StudentTextEncoder, self).__init__()\n",
        "        self.token_embedding = nn.Embedding(vocab_size, output_dim)\n",
        "        self.positional_embedding = nn.Parameter(torch.zeros(context_length, output_dim))\n",
        "        nn.init.normal_(self.positional_embedding, std=0.01)\n",
        "        encoder_layer = nn.TransformerEncoderLayer(d_model=output_dim, nhead=8)\n",
        "        self.transformer = nn.TransformerEncoder(encoder_layer, num_layers=2)\n",
        "        self.ln_final = nn.LayerNorm(output_dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: (batch_size, seq_len)\n",
        "        x = self.token_embedding(x) + self.positional_embedding  # (batch_size, seq_len, output_dim)\n",
        "        x = x.permute(1, 0, 2)  # (seq_len, batch_size, output_dim)\n",
        "        x = self.transformer(x)\n",
        "        x = x.permute(1, 0, 2)  # (batch_size, seq_len, output_dim)\n",
        "        x = self.ln_final(x)\n",
        "        x = x.mean(dim=1)  # Mean pooling over the sequence length\n",
        "        x = x / x.norm(dim=-1, keepdim=True)  # Normalize to unit length\n",
        "        return x  # (batch_size, output_dim)\n",
        "\n"
      ],
      "metadata": {
        "id": "Bu5lXd8ehL2K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Prepare the MSCOCO Data Loaders"
      ],
      "metadata": {
        "id": "h_cfc6FcltX9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from torchvision import transforms, datasets\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "\n",
        "# Preprocessing transforms\n",
        "transform = transforms.Compose([\n",
        "    transforms.Resize((input_resolution, input_resolution)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
        "                         std=(0.26862954, 0.26130258, 0.27577711))\n",
        "])\n",
        "\n",
        "# Custom dataset to select one caption per image\n",
        "class CocoDataset(Dataset):\n",
        "    def __init__(self, root, annFile, transform=None):\n",
        "        self.dataset = datasets.CocoCaptions(root=root, annFile=annFile, transform=transform)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, captions = self.dataset[idx]\n",
        "        # Select the first caption\n",
        "        caption = captions[0]\n",
        "        # Tokenize the caption\n",
        "        text = clip.tokenize(caption, context_length=context_length)[0]\n",
        "        return image, text\n",
        "\n",
        "# Paths to images and annotations\n",
        "train_img_dir = os.path.join(data_dir, 'train2014')\n",
        "train_ann_file = os.path.join(data_dir, 'annotations', 'captions_train2014.json')\n",
        "\n",
        "# Create the training dataset and dataloader\n",
        "train_dataset = CocoDataset(root=train_img_dir, annFile=train_ann_file, transform=transform)\n",
        "\n",
        "train_dataloader = DataLoader(\n",
        "    train_dataset,\n",
        "    batch_size=64,  # batch size\n",
        "    shuffle=True,\n",
        "    num_workers=2,\n",
        "    pin_memory=True\n",
        ")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dGosFCtHhMMi",
        "outputId": "1f3e23fb-4d56-496a-b0c9-0395529ac90f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.62s)\n",
            "creating index...\n",
            "index created!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the Contrastive Loss Function Using Logit"
      ],
      "metadata": {
        "id": "KijNXkynlvLL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn.functional as F\n",
        "\n",
        "\n",
        "def contrastive_loss(image_features, text_features, temperature=0.07):\n",
        "    # Normalize features\n",
        "    image_features = image_features / image_features.norm(dim=-1, keepdim=True)\n",
        "    text_features = text_features / text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    # Compute logits\n",
        "    logits_per_image = image_features @ text_features.t() / temperature\n",
        "    logits_per_text = logits_per_image.t()\n",
        "\n",
        "    # Labels\n",
        "    batch_size = image_features.size(0)\n",
        "    labels = torch.arange(batch_size, device=image_features.device)\n",
        "\n",
        "    # Cross entropy loss\n",
        "    loss_image = F.cross_entropy(logits_per_image, labels)\n",
        "    loss_text = F.cross_entropy(logits_per_text, labels)\n",
        "    loss = (loss_image + loss_text) / 2\n",
        "\n",
        "    return loss\n",
        "\n"
      ],
      "metadata": {
        "id": "pc-lmP-1hMjm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Set Up the Training Loop"
      ],
      "metadata": {
        "id": "BijGiufjlw8K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate student models\n",
        "student_image_encoder = StudentImageEncoder(output_dim=1024).to(device)\n",
        "student_text_encoder = StudentTextEncoder(vocab_size, context_length, output_dim=1024).to(device)\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = torch.optim.Adam(\n",
        "    list(student_image_encoder.parameters()) + list(student_text_encoder.parameters()),\n",
        "    lr=1e-4\n",
        ")\n"
      ],
      "metadata": {
        "id": "UyA-cl0LhM6g",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b1e3bad-a96a-49f1-e936-c9942389e026"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=None`.\n",
            "  warnings.warn(msg)\n",
            "/usr/local/lib/python3.10/dist-packages/torch/nn/modules/transformer.py:379: UserWarning: enable_nested_tensor is True, but self.use_nested_tensor is False because encoder_layer.self_attn.batch_first was not True(use batch_first for better inference performance)\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train the Student Model"
      ],
      "metadata": {
        "id": "0isNcb4ClyWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Training Loop\n",
        "num_epochs = 5  # the number of epochs\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    student_image_encoder.train()\n",
        "    student_text_encoder.train()\n",
        "    total_loss = 0.0\n",
        "\n",
        "    for batch_idx, (images, texts) in enumerate(train_dataloader):\n",
        "        images = images.to(device)\n",
        "        texts = texts.to(device)\n",
        "\n",
        "        # Teacher outputs\n",
        "        with torch.no_grad():\n",
        "            teacher_image_features = model.encode_image(images)\n",
        "            teacher_text_features = model.encode_text(texts)\n",
        "\n",
        "        # Student outputs\n",
        "        student_image_features = student_image_encoder(images).to(teacher_image_features.dtype)\n",
        "        student_text_features = student_text_encoder(texts).to(teacher_text_features.dtype)\n",
        "\n",
        "        # Compute Contrastive Loss between student features and teacher features\n",
        "        loss_image = contrastive_loss(student_image_features, teacher_text_features)\n",
        "        loss_text = contrastive_loss(student_text_features, teacher_image_features)\n",
        "        loss = (loss_image + loss_text) / 2\n",
        "\n",
        "        # Backpropagation\n",
        "        optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "        if batch_idx % 100 == 0:\n",
        "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx}/{len(train_dataloader)}], Loss: {loss.item():.4f}\")\n",
        "\n",
        "    avg_loss = total_loss / len(train_dataloader)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Average Loss: {avg_loss:.4f}\")\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vD-4WihtsZHe",
        "outputId": "aa615eb2-a74d-47a3-8e70-3b7d57053f25"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch [1/5], Step [0/1294], Loss: 4.1836\n",
            "Epoch [1/5], Step [100/1294], Loss: 2.7109\n",
            "Epoch [1/5], Step [200/1294], Loss: 2.2910\n",
            "Epoch [1/5], Step [300/1294], Loss: 2.1309\n",
            "Epoch [1/5], Step [400/1294], Loss: 2.0586\n",
            "Epoch [1/5], Step [500/1294], Loss: 2.0000\n",
            "Epoch [1/5], Step [600/1294], Loss: 1.7734\n",
            "Epoch [1/5], Step [700/1294], Loss: 1.9160\n",
            "Epoch [1/5], Step [800/1294], Loss: 1.7598\n",
            "Epoch [1/5], Step [900/1294], Loss: 1.8320\n",
            "Epoch [1/5], Step [1000/1294], Loss: 1.6387\n",
            "Epoch [1/5], Step [1100/1294], Loss: 1.7070\n",
            "Epoch [1/5], Step [1200/1294], Loss: 1.7334\n",
            "Epoch [1/5], Average Loss: 2.0444\n",
            "Epoch [2/5], Step [0/1294], Loss: 1.4980\n",
            "Epoch [2/5], Step [100/1294], Loss: 1.6182\n",
            "Epoch [2/5], Step [200/1294], Loss: 1.5879\n",
            "Epoch [2/5], Step [300/1294], Loss: 1.6855\n",
            "Epoch [2/5], Step [400/1294], Loss: 1.6748\n",
            "Epoch [2/5], Step [500/1294], Loss: 1.5107\n",
            "Epoch [2/5], Step [600/1294], Loss: 1.5361\n",
            "Epoch [2/5], Step [700/1294], Loss: 1.6406\n",
            "Epoch [2/5], Step [800/1294], Loss: 1.5654\n",
            "Epoch [2/5], Step [900/1294], Loss: 1.4727\n",
            "Epoch [2/5], Step [1000/1294], Loss: 1.4854\n",
            "Epoch [2/5], Step [1100/1294], Loss: 1.2549\n",
            "Epoch [2/5], Step [1200/1294], Loss: 1.4199\n",
            "Epoch [2/5], Average Loss: 1.5322\n",
            "Epoch [3/5], Step [0/1294], Loss: 1.3389\n",
            "Epoch [3/5], Step [100/1294], Loss: 1.3193\n",
            "Epoch [3/5], Step [200/1294], Loss: 1.3359\n",
            "Epoch [3/5], Step [300/1294], Loss: 1.3447\n",
            "Epoch [3/5], Step [400/1294], Loss: 1.5088\n",
            "Epoch [3/5], Step [500/1294], Loss: 1.3281\n",
            "Epoch [3/5], Step [600/1294], Loss: 1.1709\n",
            "Epoch [3/5], Step [700/1294], Loss: 1.3945\n",
            "Epoch [3/5], Step [800/1294], Loss: 1.2891\n",
            "Epoch [3/5], Step [900/1294], Loss: 1.3457\n",
            "Epoch [3/5], Step [1000/1294], Loss: 1.3984\n",
            "Epoch [3/5], Step [1100/1294], Loss: 1.4727\n",
            "Epoch [3/5], Step [1200/1294], Loss: 1.2793\n",
            "Epoch [3/5], Average Loss: 1.3386\n",
            "Epoch [4/5], Step [0/1294], Loss: 1.2402\n",
            "Epoch [4/5], Step [100/1294], Loss: 1.2598\n",
            "Epoch [4/5], Step [200/1294], Loss: 1.1484\n",
            "Epoch [4/5], Step [300/1294], Loss: 1.2217\n",
            "Epoch [4/5], Step [400/1294], Loss: 1.2129\n",
            "Epoch [4/5], Step [500/1294], Loss: 1.1445\n",
            "Epoch [4/5], Step [600/1294], Loss: 1.2305\n",
            "Epoch [4/5], Step [700/1294], Loss: 1.1484\n",
            "Epoch [4/5], Step [800/1294], Loss: 1.2158\n",
            "Epoch [4/5], Step [900/1294], Loss: 1.2617\n",
            "Epoch [4/5], Step [1000/1294], Loss: 1.1953\n",
            "Epoch [4/5], Step [1100/1294], Loss: 1.1172\n",
            "Epoch [4/5], Step [1200/1294], Loss: 1.1084\n",
            "Epoch [4/5], Average Loss: 1.2023\n",
            "Epoch [5/5], Step [0/1294], Loss: 1.1895\n",
            "Epoch [5/5], Step [100/1294], Loss: 1.0449\n",
            "Epoch [5/5], Step [200/1294], Loss: 1.0430\n",
            "Epoch [5/5], Step [300/1294], Loss: 1.0215\n",
            "Epoch [5/5], Step [400/1294], Loss: 0.9453\n",
            "Epoch [5/5], Step [500/1294], Loss: 1.0078\n",
            "Epoch [5/5], Step [600/1294], Loss: 1.0742\n",
            "Epoch [5/5], Step [700/1294], Loss: 1.0537\n",
            "Epoch [5/5], Step [800/1294], Loss: 0.8760\n",
            "Epoch [5/5], Step [900/1294], Loss: 0.9810\n",
            "Epoch [5/5], Step [1000/1294], Loss: 0.9771\n",
            "Epoch [5/5], Step [1100/1294], Loss: 1.2158\n",
            "Epoch [5/5], Step [1200/1294], Loss: 1.1016\n",
            "Epoch [5/5], Average Loss: 1.0901\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Evaluate the Trained Student Model"
      ],
      "metadata": {
        "id": "Q_tN8A1Cyzzj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import Dataset\n",
        "import clip\n",
        "import os\n",
        "import numpy as np\n",
        "\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "input_resolution = 224\n",
        "context_length = 77\n",
        "\n",
        "# Evaluation transforms (same as training)\n",
        "eval_transform = transforms.Compose([\n",
        "    transforms.Resize((input_resolution, input_resolution)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=(0.48145466, 0.4578275, 0.40821073),\n",
        "                         std=(0.26862954, 0.26130258, 0.27577711))\n",
        "])\n",
        "\n",
        "class CocoEvalDataset(Dataset):\n",
        "    def __init__(self, root, annFile, transform=None):\n",
        "        self.dataset = datasets.CocoCaptions(root=root, annFile=annFile, transform=transform)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image, captions = self.dataset[idx]\n",
        "        # Pick just the first caption to avoid irregular batch shapes\n",
        "        caption = captions[0]\n",
        "        return image, caption\n",
        "\n",
        "\n",
        "# Paths for validation\n",
        "val_img_dir = os.path.join(data_dir, 'val2014')\n",
        "val_ann_file = os.path.join(data_dir, 'annotations', 'captions_val2014.json')\n",
        "\n",
        "val_dataset = CocoEvalDataset(root=val_img_dir, annFile=val_ann_file, transform=eval_transform)\n",
        "val_dataloader = DataLoader(val_dataset, batch_size=64, shuffle=False, num_workers=2)\n",
        "\n",
        "student_image_encoder.eval()\n",
        "student_text_encoder.eval()\n",
        "\n",
        "all_image_features = []\n",
        "all_text_features = []\n",
        "all_captions = []  # Store captions for each image in order\n",
        "\n",
        "with torch.no_grad():\n",
        "    for images, captions in val_dataloader:\n",
        "        images = images.to(device)\n",
        "        # Tokenize captions here\n",
        "        texts = clip.tokenize(captions, context_length=context_length).to(device)\n",
        "\n",
        "        image_feats = student_image_encoder(images)\n",
        "        text_feats = student_text_encoder(texts)\n",
        "\n",
        "\n",
        "        # Normalize\n",
        "        image_feats = image_feats / image_feats.norm(dim=-1, keepdim=True)\n",
        "        text_feats = text_feats / text_feats.norm(dim=-1, keepdim=True)\n",
        "\n",
        "        all_image_features.append(image_feats.cpu())\n",
        "        all_text_features.append(text_feats.cpu())\n",
        "        all_captions.extend(captions)\n",
        "\n",
        "all_image_features = torch.cat(all_image_features, dim=0)  # (N, 512)\n",
        "all_text_features = torch.cat(all_text_features, dim=0)    # (N, 512)\n",
        "\n",
        "# Compute similarity matrix\n",
        "# image-to-text similarity: each image vs all texts\n",
        "sim_matrix = all_image_features @ all_text_features.t()  # (N, N)\n",
        "\n",
        "# Function to compute recall@K\n",
        "def compute_recall(sim_matrix, k=1):\n",
        "    # sim_matrix[i, j]: similarity of image i and text j\n",
        "    # For each image i, we find where the correct text ranks\n",
        "    # Here we matched each image with its own text at the same index\n",
        "    # If we have multiple captions per image and want a more robust metric,\n",
        "    # we assume the first caption corresponds directly.\n",
        "    ranks = []\n",
        "    n = sim_matrix.size(0)\n",
        "    for i in range(n):\n",
        "        # Sort texts by similarity to image i\n",
        "        sorted_indices = torch.argsort(sim_matrix[i], descending=True)\n",
        "        rank = (sorted_indices == i).nonzero(as_tuple=True)[0].item()\n",
        "        ranks.append(rank)\n",
        "    ranks = torch.tensor(ranks)\n",
        "    recall = (ranks < k).float().mean().item()\n",
        "    return recall\n",
        "\n",
        "r1 = compute_recall(sim_matrix, k=1)\n",
        "r5 = compute_recall(sim_matrix, k=5)\n",
        "r10 = compute_recall(sim_matrix, k=10)\n",
        "\n",
        "print(\"Image-to-Text Retrieval:\")\n",
        "print(f\"Recall@1: {r1*100:.2f}%\")\n",
        "print(f\"Recall@5: {r5*100:.2f}%\")\n",
        "print(f\"Recall@10: {r10*100:.2f}%\")\n",
        "\n",
        "# For text-to-image retrieval, we do the same but transpose the matrix\n",
        "# and consider each text in row i and find its image in column i.\n",
        "sim_matrix_t2i = sim_matrix.t()  # (N, N)\n",
        "\n",
        "r1_t2i = compute_recall(sim_matrix_t2i, k=1)\n",
        "r5_t2i = compute_recall(sim_matrix_t2i, k=5)\n",
        "r10_t2i = compute_recall(sim_matrix_t2i, k=10)\n",
        "\n",
        "print(\"Text-to-Image Retrieval:\")\n",
        "print(f\"Recall@1: {r1_t2i*100:.2f}%\")\n",
        "print(f\"Recall@5: {r5_t2i*100:.2f}%\")\n",
        "print(f\"Recall@10: {r10_t2i*100:.2f}%\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0rAEobw-2Xv0",
        "outputId": "81e4f5c0-2d6a-4cc5-adf5-381bdd71c4c7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading annotations into memory...\n",
            "Done (t=0.30s)\n",
            "creating index...\n",
            "index created!\n",
            "Image-to-Text Retrieval:\n",
            "Recall@1: 0.39%\n",
            "Recall@5: 1.77%\n",
            "Recall@10: 3.16%\n",
            "Text-to-Image Retrieval:\n",
            "Recall@1: 0.67%\n",
            "Recall@5: 2.55%\n",
            "Recall@10: 4.28%\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "-nQFuNGG2YJv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "EffHONKe_bfb"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}